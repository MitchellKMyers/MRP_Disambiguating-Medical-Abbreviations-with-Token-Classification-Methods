{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CRF Tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers,CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn_crfsuite import metrics\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "umn_df_ner = pd.read_parquet('umn_df_for_CRFner_40_0725.parquet')\n",
    "medal_df_ner = pd.read_parquet('medal_df_for_CRFner_40_0725.parquet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df_wNER = umn_df_ner.copy()\n",
    "#df_wNER = medal_df_ner.copy()\n",
    "all_abvs_list = df_wNER.ABV_final.explode().unique()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature creation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.isABV()': word in all_abvs_list,\n",
    "        'word.lemmatize()': lemmatizer.lemmatize(word),\n",
    "        'word.length()': len(word)\n",
    "        }\n",
    "\n",
    " \n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for label in sent]\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array([sent2features(s) for s in df_wNER['TEXT_clean_nostp']])\n",
    "y = np.array(df_wNER['NER_labels_words'].values)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test,y_test, test_size=0.3, random_state=1) \n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((4732,), (828,))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False,\n",
    "    max_linesearch=100,\n",
    "    verbose=True,  \n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "crf.fit(X_train,y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "pred = crf.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "true_labels = [list(x) for x in y_test]\n",
    "true_predictions = pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from seqeval.metrics import f1_score as seq_f1\n",
    "from seqeval.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "f1_actual = np.round(seq_f1(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )\n",
    "pre_actual = np.round(precision_score(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )\n",
    "rec_actual = np.round(recall_score(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "f1_actual, pre_actual, rec_actual "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(75.92, 81.45, 75.44)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class_report = classification_report(true_labels, true_predictions, output_dict=True )\n",
    "\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "weights = []\n",
    "\n",
    "for lab in class_report:\n",
    "  if lab not in ['micro avg', 'macro avg','weighted avg', 'A_word']:\n",
    "    f1s.append(class_report[lab]['f1-score'])\n",
    "    precs.append(class_report[lab]['precision'])\n",
    "    recs.append(class_report[lab]['recall'])\n",
    "    weights.append(class_report[lab]['support'])\n",
    "\n",
    "np.average(f1s, weights=weights), np.average(precs, weights=weights), np.average(recs, weights=weights)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BiLSTM Tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df_ner = pd.read_parquet('medal_df_max500v2_for_ner_1005_0726.parquet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#if MeDAL:\n",
    "#df_ner['TEXT_clean_nostp'] = df_ner['TEXT_clean_nostp'].apply(lambda row: [word.lower() for word in row])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "vocab = df_ner.TEXT_clean_nostp.explode().unique()\n",
    "vocab = np.append(vocab, 'ENDPAD')\n",
    "print(vocab)\n",
    "unique_tags = df_ner.NER_labels_words.explode().unique()\n",
    "unique_tags = np.append(unique_tags, 'ENDPAD')\n",
    "print(len(unique_tags))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "tag2idx = {t: i for i, t in enumerate(unique_tags)}\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['reduced' 'coenzyme' 'qcytochrome' ... 'origamilike' 'normalnormal'\n",
      " 'ENDPAD']\n",
      "1007\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len = 115\n",
    "\n",
    "data = [[word2idx[w] for w in x] for x in df_ner['TEXT_clean_nostp']]\n",
    "data = pad_sequences(data, maxlen=max_len, padding='post', value=word2idx['ENDPAD'])\n",
    "\n",
    "tags = [[tag2idx[w] for w in x] for x in df_ner['NER_labels_words']]\n",
    "tags = pad_sequences(tags, maxlen=max_len, padding='post', value=tag2idx['ENDPAD'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, tags, test_size=0.2,random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test,y_test, test_size=0.3, random_state=1) \n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((58556, 115), (10248, 115))"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model foundation from: https://colab.research.google.com/drive/1mnz-P30CLxrxQ0yyqpcLwVJgi7e59shi?usp=sharing\n",
    "\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "in_dim = len(vocab)\n",
    "in_len = max_len\n",
    "n_tags = len(unique_tags)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=in_dim, output_dim=256, input_length=in_len))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "# Add BiLSTM\n",
    "model.add(Bidirectional(LSTM(units=256, return_sequences=True), merge_mode = 'concat'))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "\n",
    "# Add timeDistributed Layer\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "\n",
    "#Optimiser \n",
    "adam = Adam(lr=0.005)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set class weights"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#change class weights\n",
    "class_weights = {i:100 for i in range(n_tags)}\n",
    "class_weights[0] = 1\n",
    "class_weights[n_tags-1] = 0\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=30, validation_data=(X_val, y_val), class_weight=class_weights)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from seqeval.metrics import f1_score as seq_f1\n",
    "from seqeval.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "def get_metrics(preds,labels):\n",
    "    true_predictions = [\n",
    "    [list(tag2idx.keys())[p] for (p, l) in zip(prediction, label) if l != 1006]\n",
    "    for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [list(tag2idx.keys())[l] for (p, l) in zip(prediction, label) if l != 1006]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "\n",
    "    f1_actual = np.round(seq_f1(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )\n",
    "    pre_actual = np.round(precision_score(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )\n",
    "    rec_actual = np.round(recall_score(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )\n",
    "\n",
    "    print('Macro Performance (F1, Precision, Recall):\\t', f1_actual, pre_actual, rec_actual)\n",
    "\n",
    "    class_report = classification_report(true_labels, true_predictions, output_dict=True )\n",
    "\n",
    "    f1s = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "    weights = []\n",
    "\n",
    "    for lab in class_report:\n",
    "        if lab not in ['micro avg', 'macro avg','weighted avg', 'A_word']:\n",
    "            f1s.append(class_report[lab]['f1-score'])\n",
    "            precs.append(class_report[lab]['precision'])\n",
    "            recs.append(class_report[lab]['recall'])\n",
    "            weights.append(class_report[lab]['support'])\n",
    "\n",
    "    print('Weighted Performance (F1, Precision, Recall):\\t', (np.average(f1s, weights=weights), np.average(precs, weights=weights), np.average(recs, weights=weights)))\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for prediction, label in zip(preds, labels):\n",
    "        preds = []\n",
    "        labs = []\n",
    "        for (p, l) in zip(prediction, label):\n",
    "            if l != tag2idx['ENDPAD']:\n",
    "                if p > 0:\n",
    "                    preds.append('ABV')\n",
    "                elif p == 0:\n",
    "                    preds.append('word')\n",
    "\n",
    "                if l > 0:\n",
    "                    labs.append('ABV')\n",
    "                elif l == 0:\n",
    "                    labs.append('word')\n",
    "        true_predictions.append(preds)\n",
    "        true_labels.append(labs)\n",
    "\n",
    "    f1_actual = np.round(seq_f1(true_labels, true_predictions, average=None, scheme='token' ) * 100, 2 )\n",
    "    pre_actual = np.round(precision_score(true_labels, true_predictions, average=None, scheme='token' ) * 100, 2 )\n",
    "    rec_actual = np.round(recall_score(true_labels, true_predictions, average=None, scheme='token' ) * 100, 2 )\n",
    "\n",
    "    print('ABV Identification Performance (F1, Precision, Recall):\\t', f1_actual, pre_actual, rec_actual)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "true_labels_ = y_test\n",
    "true_predictions_ = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "true_labels_ = [list(x) for x in true_labels_]\n",
    "true_predictions_ = [list(x) for x in true_predictions_]\n",
    "\n",
    "get_metrics(true_predictions_, true_labels_)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}