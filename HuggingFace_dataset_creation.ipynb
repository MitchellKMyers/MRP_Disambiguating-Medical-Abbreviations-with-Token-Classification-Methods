{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#df_name = 'umn_df_for_CRFner_40_0725.parquet'  # UMN-40 in study\n",
    "df_name = 'medal_df_max500v2_for_ner_1005_0726.parquet' # MeDAL-RTE in study\n",
    "#df_name = 'medal_df_for_ner_1005_0727.parquet' # MeDAL in study\n",
    "#df_name = 'umn_df_for_ner_203_0727.parquet' # UMN in study\n",
    "#df_name = 'medal_df_max500v2_for_CRFner_40_0803.parquet' # MeDAL-RTE-40 in study\n",
    "\n",
    "ner_df = pd.read_parquet(df_name)\n",
    "\n",
    "ner_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT_clean_nostp</th>\n",
       "      <th>updated_noStp_LOCATION</th>\n",
       "      <th>LABEL_final</th>\n",
       "      <th>ABV_final</th>\n",
       "      <th>NER_labels</th>\n",
       "      <th>NER_labels_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[reduced, coenzyme, qcytochrome, c, reductase,...</td>\n",
       "      <td>[12, 87, 98, 108]</td>\n",
       "      <td>[vesicles, vesicles, alone, energy]</td>\n",
       "      <td>[LDV, LDV, CT, SE]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, NA_word, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[EP, techniques, employed, examine, nature, re...</td>\n",
       "      <td>[0, 28]</td>\n",
       "      <td>[electrophysiological, minutes]</td>\n",
       "      <td>[EP, T2]</td>\n",
       "      <td>[4, 1005, 1005, 1005, 1005, 1005, 1005, 1005, ...</td>\n",
       "      <td>[electrophysiological, NA_word, NA_word, NA_wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[excretion, enzyme, gammaglutamyltranspeptidas...</td>\n",
       "      <td>[39]</td>\n",
       "      <td>[necrosis]</td>\n",
       "      <td>[CN]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, NA_word, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[peptidases, activities, compared, human, leuc...</td>\n",
       "      <td>[14, 19]</td>\n",
       "      <td>[active, alkaline]</td>\n",
       "      <td>[AS, ALP]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, NA_word, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[activity, three, known, conducting, systems, ...</td>\n",
       "      <td>[43]</td>\n",
       "      <td>[pulse]</td>\n",
       "      <td>[PP]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, NA_word, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73191</th>\n",
       "      <td>[tremendous, effort, accessing, liquidphase, s...</td>\n",
       "      <td>[16, 37]</td>\n",
       "      <td>[relaxation, twodimensional]</td>\n",
       "      <td>[EC50, 2DE]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, NA_word, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73192</th>\n",
       "      <td>[glass, transition, binodals, asymmetric, bina...</td>\n",
       "      <td>[9, 38, 68, 74]</td>\n",
       "      <td>[approach, agreement, effective, effective]</td>\n",
       "      <td>[RPA, PA, ERP, ERP]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, NA_word, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73193</th>\n",
       "      <td>[mechanical, properties, thermally, excited, 2...</td>\n",
       "      <td>[4, 23]</td>\n",
       "      <td>[twodimensional, study]</td>\n",
       "      <td>[2DE, T0]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 397, 1005, 1005, 1005...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, twodimens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73194</th>\n",
       "      <td>[approaches, mapping, time, series, networks, ...</td>\n",
       "      <td>[102]</td>\n",
       "      <td>[nodes]</td>\n",
       "      <td>[NO]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, NA_word, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73195</th>\n",
       "      <td>[flexible, mechanical, deformation, test, syst...</td>\n",
       "      <td>[6, 13, 17, 30]</td>\n",
       "      <td>[microwave, microwave, line, line]</td>\n",
       "      <td>[MW, MW, L1, L1]</td>\n",
       "      <td>[1005, 1005, 1005, 1005, 1005, 1005, 420, 1005...</td>\n",
       "      <td>[NA_word, NA_word, NA_word, NA_word, NA_word, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73196 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        TEXT_clean_nostp  \\\n",
       "0      [reduced, coenzyme, qcytochrome, c, reductase,...   \n",
       "1      [EP, techniques, employed, examine, nature, re...   \n",
       "2      [excretion, enzyme, gammaglutamyltranspeptidas...   \n",
       "3      [peptidases, activities, compared, human, leuc...   \n",
       "4      [activity, three, known, conducting, systems, ...   \n",
       "...                                                  ...   \n",
       "73191  [tremendous, effort, accessing, liquidphase, s...   \n",
       "73192  [glass, transition, binodals, asymmetric, bina...   \n",
       "73193  [mechanical, properties, thermally, excited, 2...   \n",
       "73194  [approaches, mapping, time, series, networks, ...   \n",
       "73195  [flexible, mechanical, deformation, test, syst...   \n",
       "\n",
       "      updated_noStp_LOCATION                                  LABEL_final  \\\n",
       "0          [12, 87, 98, 108]          [vesicles, vesicles, alone, energy]   \n",
       "1                    [0, 28]              [electrophysiological, minutes]   \n",
       "2                       [39]                                   [necrosis]   \n",
       "3                   [14, 19]                           [active, alkaline]   \n",
       "4                       [43]                                      [pulse]   \n",
       "...                      ...                                          ...   \n",
       "73191               [16, 37]                 [relaxation, twodimensional]   \n",
       "73192        [9, 38, 68, 74]  [approach, agreement, effective, effective]   \n",
       "73193                [4, 23]                      [twodimensional, study]   \n",
       "73194                  [102]                                      [nodes]   \n",
       "73195        [6, 13, 17, 30]           [microwave, microwave, line, line]   \n",
       "\n",
       "                 ABV_final                                         NER_labels  \\\n",
       "0       [LDV, LDV, CT, SE]  [1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...   \n",
       "1                 [EP, T2]  [4, 1005, 1005, 1005, 1005, 1005, 1005, 1005, ...   \n",
       "2                     [CN]  [1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...   \n",
       "3                [AS, ALP]  [1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...   \n",
       "4                     [PP]  [1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...   \n",
       "...                    ...                                                ...   \n",
       "73191          [EC50, 2DE]  [1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...   \n",
       "73192  [RPA, PA, ERP, ERP]  [1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...   \n",
       "73193            [2DE, T0]  [1005, 1005, 1005, 1005, 397, 1005, 1005, 1005...   \n",
       "73194                 [NO]  [1005, 1005, 1005, 1005, 1005, 1005, 1005, 100...   \n",
       "73195     [MW, MW, L1, L1]  [1005, 1005, 1005, 1005, 1005, 1005, 420, 1005...   \n",
       "\n",
       "                                        NER_labels_words  \n",
       "0      [NA_word, NA_word, NA_word, NA_word, NA_word, ...  \n",
       "1      [electrophysiological, NA_word, NA_word, NA_wo...  \n",
       "2      [NA_word, NA_word, NA_word, NA_word, NA_word, ...  \n",
       "3      [NA_word, NA_word, NA_word, NA_word, NA_word, ...  \n",
       "4      [NA_word, NA_word, NA_word, NA_word, NA_word, ...  \n",
       "...                                                  ...  \n",
       "73191  [NA_word, NA_word, NA_word, NA_word, NA_word, ...  \n",
       "73192  [NA_word, NA_word, NA_word, NA_word, NA_word, ...  \n",
       "73193  [NA_word, NA_word, NA_word, NA_word, twodimens...  \n",
       "73194  [NA_word, NA_word, NA_word, NA_word, NA_word, ...  \n",
       "73195  [NA_word, NA_word, NA_word, NA_word, NA_word, ...  \n",
       "\n",
       "[73196 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_dataset,test_dataset = train_test_split(ner_df[['TEXT_clean_nostp', 'updated_noStp_LOCATION', 'NER_labels']], test_size=0.2,random_state=1)\n",
    "test_dataset, val_dataset = train_test_split(test_dataset, test_size=0.3, random_state=1) \n",
    "train_dataset = Dataset(pa.Table.from_pandas(train_dataset))\n",
    "val_dataset = Dataset(pa.Table.from_pandas(val_dataset))\n",
    "test_dataset = Dataset(pa.Table.from_pandas(test_dataset))\n",
    "\n",
    "import datasets\n",
    "final_dataset = datasets.DatasetDict({'train':train_dataset, 'validation':val_dataset, 'test':test_dataset})\n",
    "\n",
    "final_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set BERT model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "task = \"ner\" \n",
    "#model_checkpoint = \"dmis-lab/biobert-v1.1\" #biobert\n",
    "#model_checkpoint = 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12' #bluebert\n",
    "#model_checkpoint = 'NLP4H/ms_bert'\n",
    "#model_checkpoint = 'distilbert-base-uncased'\n",
    "#model_checkpoint = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "model_checkpoint = 'allenai/scibert_scivocab_uncased'\n",
    "batch_size = 8\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizing and aligning data\n",
    "since tokenization can break down words into subwords, we need to align the labels so that each subword is assigned the correct label as well\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## code from: https://huggingface.co/docs/transformers/tasks/token_classification\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"TEXT_clean_nostp\"], is_split_into_words=True, \n",
    "    )\n",
    "    labels = []\n",
    "    label = examples['NER_labels']\n",
    "   \n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        # Special tokens have a word id that is None. We set the label to -100 so they are automatically ignored in the loss function.\n",
    "\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        # We set the label for the first token of each word.\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(label[word_idx])\n",
    "        # For the other tokens in a word, we set the label to either the current label\n",
    "        else:\n",
    "            labels.append(label[word_idx])\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    \n",
    "    tokenized_inputs[\"word_ids\"] = word_ids\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized_datasets = final_dataset.map(tokenize_and_align_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save to disk for model training. I ran on COLAB for GPU usage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#tokenized_datasets.save_to_disk('umn_40_tokenized_dataset_SciBERT_addedtokens_0823_t1v1.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#df_name = 'umn_df_for_CRFner_40_0725.parquet'  # UMN-40 in study\n",
    "df_name = 'medal_df_max500v2_for_ner_1005_0726.parquet' # MeDAL-RTE in study\n",
    "#df_name = 'medal_df_for_ner_1005_0727.parquet' # MeDAL in study\n",
    "#df_name = 'umn_df_for_ner_203_0727.parquet' # UMN in study\n",
    "#df_name = 'medal_df_max500v2_for_CRFner_40_0803.parquet' # MeDAL-RTE-40 in study\n",
    "text_class_df = pd.read_parquet(df_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split data prior to text classification dataset creation in order to ensure proper comparison"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "train_dataset,test_dataset = train_test_split(text_class_df, test_size=0.2,random_state=1)\n",
    "test_dataset, val_dataset = train_test_split(test_dataset, test_size=0.3, random_state=1) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "unique_labs = text_class_df.LABEL_final.explode().unique()\n",
    "#np.save('unique_labs_umn_40_txtclass_0824.npy', unique_labs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_for_text_classification(df):\n",
    "    new_df = pd.DataFrame(columns=['ABSTRACT_ID', 'TEXT', 'ABV', 'LABEL'])\n",
    "    for i,(row_id,row) in enumerate(df.iterrows()):\n",
    "        #print(i, row_id)\n",
    "        #f i % 5000 == 0: print(i)\n",
    "        for idx, loc_idx in enumerate(row['updated_noStp_LOCATION']):\n",
    "            #print(loc_idx)\n",
    "            new_row = {'ABSTRACT_ID': i,'TEXT': row['TEXT_clean_nostp'][max(0,loc_idx-8):loc_idx+9], 'ABV': row['ABV_final'][idx], 'LABEL':row['LABEL_final'][idx] }\n",
    "            new_df = new_df.append(new_row, ignore_index=True)\n",
    "        #print(new_df)\n",
    "    return new_df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = split_for_text_classification(train_dataset)\n",
    "print('done')\n",
    "test_dataset = split_for_text_classification(test_dataset)\n",
    "print('done')\n",
    "val_dataset = split_for_text_classification(val_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = Dataset(pa.Table.from_pandas(train_dataset))\n",
    "val_dataset = Dataset(pa.Table.from_pandas(val_dataset))\n",
    "test_dataset = Dataset(pa.Table.from_pandas(test_dataset))\n",
    "\n",
    "import datasets\n",
    "final_dataset = datasets.DatasetDict({'train':train_dataset, 'validation':val_dataset, 'test':test_dataset})\n",
    "\n",
    "final_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model_checkpoint = \"dmis-lab/biobert-v1.1\" #biobert\n",
    "#model_checkpoint = 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12' #bluebert\n",
    "#model_checkpoint = 'NLP4H/ms_bert'\n",
    "#model_checkpoint = 'distilbert-base-uncased'\n",
    "#model_checkpoint = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "model_checkpoint = 'allenai/scibert_scivocab_uncased'\n",
    "batch_size = 8\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No need to align since this is just text classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"TEXT\"], is_split_into_words=True)\n",
    "    tokenized_inputs['label'] = list(unique_labs).index(examples['LABEL'])  \n",
    "    return tokenized_inputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized_datasets = final_dataset.map(preprocess_function)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}