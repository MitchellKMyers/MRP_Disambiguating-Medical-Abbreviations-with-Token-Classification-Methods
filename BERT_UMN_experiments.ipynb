{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### This was ran on Colab for GPU usage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip install datasets transformers seqeval"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "#model_checkpoint = \"dmis-lab/biobert-v1.1\" #biobert\n",
    "#model_checkpoint = 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12' #bluebert\n",
    "#model_checkpoint = 'NLP4H/ms_bert'\n",
    "#model_checkpoint = 'distilbert-base-uncased'\n",
    "#model_checkpoint = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "model_checkpoint = 'allenai/scibert_scivocab_uncased'\n",
    "batch_size = 8\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_datasets = load_from_disk('/content/umn_40_tokenized_dataset_SciBERT_addedtokens_0823_t1v1.json')\n",
    "label_list = np.load('/content/unique_labs_umn_40_0804.npy', allow_pickle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"umn-{model_name}-finetuned-textclass\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy = 'no'\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"umncrf-{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy = 'no'\n",
    ")\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.save_model()\n",
    "\n",
    "#on Colab:\n",
    "#!zip -r /content/drive/MyDrive/umn_203_6ep_scibert_scivocab_uncased-finetuned-ner_0823.zip /content/umnfull-scibert_scivocab_uncased-finetuned-ner"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions_ = np.argmax(predictions, axis=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Token classification performance functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from seqeval.metrics import f1_score as seq_f1\n",
    "from seqeval.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "def get_AD_performance(predictions, labels):\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    f1_actual = np.round(seq_f1(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )\n",
    "    pre_actual = np.round(precision_score(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )\n",
    "    rec_actual = np.round(recall_score(true_labels, true_predictions, average='macro', scheme='token' ) * 100, 2 )\n",
    "\n",
    "    class_report = classification_report(true_labels, true_predictions, output_dict=True )\n",
    "    f1s = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "    weights = []\n",
    "\n",
    "    for lab in class_report:\n",
    "        if lab not in ['micro avg', 'macro avg','weighted avg', 'A_word']:\n",
    "            f1s.append(class_report[lab]['f1-score'])\n",
    "            precs.append(class_report[lab]['precision'])\n",
    "            recs.append(class_report[lab]['recall'])\n",
    "            weights.append(class_report[lab]['support'])\n",
    "    \n",
    "    return (f1_actual, pre_actual, rec_actual), (np.average(f1s, weights=weights), np.average(precs, weights=weights), np.average(recs, weights=weights)) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_ID_performance(predictions, labels):\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "      preds = []\n",
    "      labs = []\n",
    "      for (p, l) in zip(prediction, label):\n",
    "        if l != -100:\n",
    "          if p < 203:\n",
    "            preds.append('ABV')\n",
    "          elif p == 203:\n",
    "            preds.append('word')\n",
    "\n",
    "          if l < 203:\n",
    "            labs.append('ABV')\n",
    "          elif l == 203:\n",
    "            labs.append('word')\n",
    "      true_predictions.append(preds)\n",
    "      true_labels.append(labs)\n",
    "\n",
    "    f1_actual_ID = np.round(seq_f1(true_labels, true_predictions, average=None, scheme='token' ) * 100, 2 )\n",
    "    pre_actual_ID = np.round(precision_score(true_labels, true_predictions, average=None, scheme='token' ) * 100, 2 )\n",
    "    rec_actual_ID = np.round(recall_score(true_labels, true_predictions, average=None, scheme='token' ) * 100, 2 )\n",
    "\n",
    "    return (f1_actual_ID, pre_actual_ID, rec_actual_ID)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "macro_scores, weighted_scores = get_AD_performance(predictions, labels)\n",
    "id_perf = get_ID_performance(predictions, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \"Second-Best\" Post processing prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tr_set = tokenized_datasets['train']\n",
    "tst_set = tokenized_datasets['test']\n",
    "unique_input_ids = {x for l in tr_set['input_ids'] for x in l}\n",
    "\n",
    "possible_labs_dict = {k:set([list(label_list).index('NA_word')]) for k in unique_input_ids}\n",
    "\n",
    "tst_set = tokenized_datasets['test']\n",
    "unique_input_ids = {x for l in tr_set['input_ids'] for x in l}\n",
    "\n",
    "possible_labs_dict = {k:set([list(label_list).index('NA_word')]) for k in unique_input_ids}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def post_process_preds(labs_dict, preds, tst_set):\n",
    "    final_preds = []\n",
    "    for words_lst, preds_lsts in zip(tst_set['input_ids'], preds):\n",
    "        #print(len(words_lst),len( preds_lsts[:len(words_lst)]))\n",
    "        updated_preds_list = []\n",
    "\n",
    "        for word_idx, pred in zip(words_lst, preds_lsts[:len(words_lst)]):\n",
    "        \n",
    "            top_two_ids = np.argsort(pred)[-2:]  # Top two labels\n",
    "            if word_idx in labs_dict.keys():\n",
    "                possible_preds = labs_dict[word_idx]\n",
    "                #print(possible_preds, top_two_ids)\n",
    "                if pred[top_two_ids[1]] - pred[top_two_ids[0]] < 0.03 and top_two_ids[0] in possible_preds: ## If second best likelihood is close enough to best than predict second best\n",
    "                    actual_pred = top_two_ids[0]\n",
    "                else:\n",
    "                    actual_pred = top_two_ids[1]\n",
    "            \n",
    "            else:\n",
    "                if pred[top_two_ids[1]] - pred[top_two_ids[0]] < 0.01:\n",
    "                    #print('yes')\n",
    "                    actual_pred = top_two_ids[0]\n",
    "                else:\n",
    "                    actual_pred = top_two_ids[1]\n",
    "\n",
    "            updated_preds_list.append(actual_pred)\n",
    "        final_preds.append(updated_preds_list)\n",
    "\n",
    "    return final_preds\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "post_preds = post_process_preds(possible_labs_dict, predictions, tst_set, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "post_preds = pad_sequences(post_preds, padding='post', value=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "macro_scores, weighted_scores = get_AD_performance(post_preds, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text classification Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions_max = np.argmax(predictions, axis=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "true_labels = [label_list[i] for i in labels ]\n",
    "true_predictions = [label_list[i] for i in predictions_max]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report, f1_score\n",
    "\n",
    "f1_actual = np.round(f1_score(true_labels, true_predictions, average='macro' ) * 100, 2 )\n",
    "pre_actual = np.round(precision_score(true_labels, true_predictions, average='macro' ) * 100, 2 )\n",
    "rec_actual = np.round(recall_score(true_labels, true_predictions, average='macro', zero_division=0 ) * 100, 2 )"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}